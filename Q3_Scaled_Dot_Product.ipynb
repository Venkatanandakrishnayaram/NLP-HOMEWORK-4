{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "id": "Xr2SGhZ4vbOK",
        "outputId": "62c7e050-c003-4e8f-f68a-dc03a7d82645"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q shape: torch.Size([1, 4, 8])\n",
            "K shape: torch.Size([1, 4, 8])\n",
            "V shape: torch.Size([1, 4, 8])\n",
            "\n",
            "Raw attention scores (no scaling):\n",
            " tensor([[-0.0146,  5.1091, -0.3921, -3.7779],\n",
            "        [ 0.4648,  0.5446, -0.7034,  0.9571],\n",
            "        [ 1.3816, -7.0338, -0.2771,  2.2690],\n",
            "        [ 0.6756,  3.8463, -1.2603, -2.3062]])\n",
            "\n",
            "Scaled attention scores (divided by ‚àöd_k):\n",
            " tensor([[-0.0052,  1.8063, -0.1386, -1.3357],\n",
            "        [ 0.1643,  0.1926, -0.2487,  0.3384],\n",
            "        [ 0.4885, -2.4868, -0.0980,  0.8022],\n",
            "        [ 0.2389,  1.3599, -0.4456, -0.8154]])\n",
            "\n",
            "‚úÖ Attention Weights (softmax over scaled scores):\n",
            " tensor([[0.1211, 0.7410, 0.1060, 0.0320],\n",
            "        [0.2577, 0.2651, 0.1705, 0.3067],\n",
            "        [0.3360, 0.0171, 0.1869, 0.4599],\n",
            "        [0.2032, 0.6235, 0.1025, 0.0708]])\n",
            "\n",
            "‚úÖ Output Vectors:\n",
            " tensor([[-1.2940, -0.1495, -0.3701, -0.2156, -0.9138,  1.0640, -0.0503,  0.6983],\n",
            "        [-0.6700,  0.9147, -0.5610,  0.1849, -0.8407,  0.5956,  0.0283,  0.5789],\n",
            "        [-0.3498,  1.4870, -0.6570,  0.4022, -0.8382,  0.3671,  0.0483,  0.5201],\n",
            "        [-1.1585,  0.0777, -0.2926, -0.0528, -0.9533,  1.1081, -0.0749,  0.6575]])\n",
            "\n",
            "Score Range Comparison:\n",
            "Unscaled scores range:   -7.033776760101318 ‚Üí 5.109094142913818\n",
            "Scaled scores range:     -2.4868156909942627 ‚Üí 1.806337594985962\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nüß† Notes for Report\\n- Formula: Attention(Q,K,V) = softmax((QK·µÄ)/‚àöd‚Çñ) V\\n- Dividing by ‚àöd‚Çñ prevents extremely large dot-products,\\n  which could cause the softmax to saturate and give unstable gradients.\\n- The attention weight matrix shows how each query attends to keys,\\n  and the weighted sum of V produces context-aware output vectors.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# ============================================\n",
        "# üß† Q3. Scaled Dot-Product Attention\n",
        "# ============================================\n",
        "\n",
        "!pip install torch --quiet\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "# --------------------------------------------\n",
        "# 1Ô∏è‚É£ Define the Scaled Dot-Product Attention\n",
        "# --------------------------------------------\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    \"\"\"\n",
        "    Computes attention(Q,K,V) = softmax((QK^T)/‚àöd_k) * V\n",
        "    Q, K, V: tensors of shape (batch, seq_len, d_k)\n",
        "    Returns: (output, attention_weights)\n",
        "    \"\"\"\n",
        "    d_k = Q.size(-1)\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    weights = F.softmax(scores, dim=-1)\n",
        "    output = torch.matmul(weights, V)\n",
        "    return output, weights\n",
        "\n",
        "# --------------------------------------------\n",
        "# 2Ô∏è‚É£ Generate Random Q, K, V Inputs\n",
        "# --------------------------------------------\n",
        "torch.manual_seed(0)\n",
        "batch_size = 1\n",
        "seq_len = 4\n",
        "d_k = 8\n",
        "\n",
        "Q = torch.randn(batch_size, seq_len, d_k)\n",
        "K = torch.randn(batch_size, seq_len, d_k)\n",
        "V = torch.randn(batch_size, seq_len, d_k)\n",
        "\n",
        "print(\"Q shape:\", Q.shape)\n",
        "print(\"K shape:\", K.shape)\n",
        "print(\"V shape:\", V.shape)\n",
        "\n",
        "# --------------------------------------------\n",
        "# 3Ô∏è‚É£ Check Softmax Stability (Before / After Scaling)\n",
        "# --------------------------------------------\n",
        "raw_scores = torch.matmul(Q, K.transpose(-2, -1))\n",
        "print(\"\\nRaw attention scores (no scaling):\\n\", raw_scores[0])\n",
        "\n",
        "scaled_scores = raw_scores / math.sqrt(d_k)\n",
        "print(\"\\nScaled attention scores (divided by ‚àöd_k):\\n\", scaled_scores[0])\n",
        "\n",
        "# --------------------------------------------\n",
        "# 4Ô∏è‚É£ Compute Attention\n",
        "# --------------------------------------------\n",
        "output, weights = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "print(\"\\n‚úÖ Attention Weights (softmax over scaled scores):\\n\", weights[0])\n",
        "print(\"\\n‚úÖ Output Vectors:\\n\", output[0])\n",
        "\n",
        "# --------------------------------------------\n",
        "# 5Ô∏è‚É£ Verify Stability ‚Äî Observe numeric range\n",
        "# --------------------------------------------\n",
        "print(\"\\nScore Range Comparison:\")\n",
        "print(\"Unscaled scores range:  \", raw_scores.min().item(), \"‚Üí\", raw_scores.max().item())\n",
        "print(\"Scaled scores range:    \", scaled_scores.min().item(), \"‚Üí\", scaled_scores.max().item())\n",
        "\n",
        "\"\"\"\n",
        "üß† Notes for Report\n",
        "- Formula: Attention(Q,K,V) = softmax((QK·µÄ)/‚àöd‚Çñ) V\n",
        "- Dividing by ‚àöd‚Çñ prevents extremely large dot-products,\n",
        "  which could cause the softmax to saturate and give unstable gradients.\n",
        "- The attention weight matrix shows how each query attends to keys,\n",
        "  and the weighted sum of V produces context-aware output vectors.\n",
        "\"\"\"\n"
      ]
    }
  ]
}